# pdt_main
## experiment_mix_env
    用get_env_list先读训练环境，再读测试环境
    用load_data_prompt先读训练集/prompt，再读测试集/prompt，共两个环境，每个环境999条轨迹/每个环境5条轨迹，每个轨迹20个时间步长
    用process_info读取两个环境的数据

    model= PromptDecisionTransformer(...)科学院自动化研究所 菁英班
    warmup
    trainer = PromptSequenceTrainer(...)
    训练阶段：
        共迭代5000轮：间隔训练两个环境
            outputs = trainer.pure_train_iteration_mix(...)
                每次走10个时间步
            每满100轮时：
                如果不微调：
                    test_eval_logs = trainer.eval_iteration_multienv(...)
                    进入评估模式，遍历每一个环境：
                        对于每一个环境，生成一个评估函数(eval_episodes)
                        如果有prompt，prompt=flatten_prompt(...)
                        对于每一个评估函数，outputs= eval_fn(self.model, prompt=self.prompt)
                如果微调：
            每满500轮时：
                train_eval_logs = trainer.eval_iteration_multienv(...)
            每满？：
                trainer.save_model
        trainer.save_model
    评估阶段：
        读取模型
            eval_logs = trainer.eval_iteration_multienv(...)

# prompt_utils
## process_info
    利用process_dataset读取两个环境的数据
## process_dataset
    normal模式，读取状态、动作、累计奖励，计算所有状态、奖励的平均值和标准差
    采样那块有用吗？
## get_batch
    读取各种参数
### fn
    读取并处理（补零、归一化）s,a,r,d,rtg等
    用于一个环境中选择batch_size(16)条轨迹
    (batch_size,seq_len,dim)


## get_env_list
    对于环境列表中的每个环境，调用gen_env读取env, max_ep_len, env_targets并存储在info元组中，
    env_list.append(env)
## gen_env
    对于cheetah_dir：
        两个方向奔跑的环境

## get_prompt
    读取各种参数
### fn
    读取并处理（补零、归一化）s,a,r,d,rtg等
    (batch_size,seq_len,dim)，batch_size=1
    只有1条轨迹

## flatten_prompt
    (batch_size*seq_len,dim)->(batch_size,seq_len,dim)


## load_data_prompt
    读取pkl数据
## get_prompt_batch
### fn
    对于每个环境，返回16个轨迹和16个prompt
    prompt, batch: (num_env*batch_size, seq_len, dim)
## eval_episodes
### fn
    循环50次，每次不计算梯度，运行ret, infos = prompt_evaluate_episode_rtg(...)



# PromptDecisionTransformer
    初始化基本同DT，多出了prompt的初始化
## forward
    同DT
    prompt_stacked_inputs:(batch_size, 3 * prompt_seq_length, hidden_size)
    时间切片是什么意思
    将原先的输入拼接上了prompt
    input:
        prompt_stacked_inputs+stacked_inputs--->(num_env*batch_size, 3*prompt_seq_length, dim)+(num_env*batch_size, 3*seq_length, dim)
        prompt_stacked_inputs_mask+stacked_inputs_mask--->(num_env*batch_size, 3*prompt_seq_length)+(num_env*batch_size, 3*seq_length)
## get_action

# PromptSequenceTrainer
## pure_train_iteration_mix
    走10个时间步：
        train_loss = self.train_step_mix(...)

## train_step_mix
    前向传播，获得预测，反向传播

# prompt_evaluate_episodes
## prompt_evaluate_episode_rtg
    最多往前走1000步：
        对于状态是否归一化的两种情况，分别选择动作